{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install together"
      ],
      "metadata": {
        "id": "zP8jL4LwSvaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from together import Together\n"
      ],
      "metadata": {
        "id": "m7CcejWTqURL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"1b35247990ac187e8443a7742235115710fa59f8cb599aac17d23fda6f897971\"  # place your api key from together ai `https://api.together.ai/settings/api-keys`\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n"
      ],
      "metadata": {
        "id": "xXT7WnO_ljBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZ4rg_WwAIRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Current Working workflow"
      ],
      "metadata": {
        "id": "H_TF5lJ0AKrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#  topic hierarchies with more granular categories\n",
        "TOPIC_HIERARCHIES = {\n",
        "    \"Knowledge\": [\n",
        "        \"Academic\", \"Research\", \"History\", \"Philosophy\", \"Science\",\n",
        "        \"Mathematics\", \"Literature\", \"Languages\"\n",
        "    ],\n",
        "    \"Technology\": [\n",
        "        \"AI/ML\", \"Software\", \"Hardware\", \"Cybersecurity\", \"Cloud\",\n",
        "        \"Blockchain\", \"IoT\", \"Data Science\", \"Programming Languages\", \"Mobile\"\n",
        "    ],\n",
        "    \"Current Affairs\": [\n",
        "        \"Politics\", \"Economics\", \"Social Issues\", \"Environment\",\n",
        "        \"International Relations\", \"Law\", \"Education\"\n",
        "    ],\n",
        "    \"Lifestyle\": [\n",
        "        \"Health\", \"Fitness\", \"Food\", \"Travel\", \"Fashion\",\n",
        "        \"Personal Finance\", \"Self Improvement\"\n",
        "    ],\n",
        "    \"Entertainment\": [\n",
        "        \"Movies\", \"TV Shows\", \"Music\", \"Gaming\", \"Sports\",\n",
        "        \"Art\", \"Books\", \"Pop Culture\"\n",
        "    ],\n",
        "    \"Professional\": [\n",
        "        \"Career\", \"Business\", \"Management\", \"Marketing\",\n",
        "        \"Entrepreneurship\", \"Industry Specific\"\n",
        "    ],\n",
        "    \"General\": [\n",
        "        \"Casual Chat\", \"Greetings\", \"Personal\", \"Questions\",\n",
        "        \"Recommendations\", \"Feedback\", \"Statement\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Few-shot examples for better query understanding\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    {\n",
        "        \"query\": \"who is pm of india\",\n",
        "        \"expanded\": \"Who is the pm of india?\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"what are his duties\",\n",
        "        \"expanded\": \"what are duties of narendra modi?\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Give me minute, I am coming back\",\n",
        "        \"expanded\": \"Give me minute, I am coming back\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"what about uk\",\n",
        "        \"expanded\": \"who is pm of uk and what are his duties? \",\n",
        "        \"topic\": \"Current Affairs-Politics\"\n",
        "    }\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "    expanded_query: Optional[str] = None\n",
        "    topic: Optional[str] = None\n",
        "    timestamp: datetime = datetime.now()\n",
        "    confidence_score: Optional[float] = None\n",
        "\n",
        "class QueryProcessor:\n",
        "    def __init__(self, api_key: str):\n",
        "        \"\"\"Initialize QueryProcessor with direct API key.\"\"\"\n",
        "        self.client = Together(api_key=api_key)\n",
        "        self.conversation_history: List[Message] = []\n",
        "        self.query_history: List[Tuple[str, str, str, float]] = []  # (query, expanded_query, topic, confidence_score)\n",
        "        self.error_count = 0\n",
        "        self.max_retries = 3\n",
        "\n",
        "    def create_expansion_prompt(self, query: str) -> str:\n",
        "        \"\"\"Create a prompt with few-shot examples and conversation context for better query expansion.\"\"\"\n",
        "        # Get recent conversation context\n",
        "        recent_context = self.conversation_history[-3:] if self.conversation_history else []\n",
        "        context_str = \"\\n\".join([\n",
        "            f\"{msg.role}: {msg.content} (Topic: {msg.topic})\"\n",
        "            for msg in recent_context\n",
        "        ])\n",
        "\n",
        "        # Get recent query history\n",
        "        recent_queries = self.query_history[-5:]\n",
        "        query_history_str = \"\\n\".join([\n",
        "            f\"Query: {q}\\nExpanded: {eq}\\nTopic: {t} (Confidence: {c:.2f})\"\n",
        "            for q, eq, t, c in recent_queries\n",
        "        ])\n",
        "\n",
        "        # Format few-shot examples\n",
        "        examples_str = \"\\n\".join([\n",
        "            f\"Query: {ex['query']}\\nExpanded: {ex['expanded']}\\nTopic: {ex['topic']}\\n\"\n",
        "            for ex in FEW_SHOT_EXAMPLES\n",
        "        ])\n",
        "\n",
        "        return f\"\"\"Given the conversation context, recent query history, and current query, expand the query to be more complete and identify its topic.\n",
        "Use these few-shot examples as a guide:\n",
        "\n",
        "{examples_str}\n",
        "\n",
        "Available topic hierarchies (format as MainTopic-SubTopic):\n",
        "{json.dumps(TOPIC_HIERARCHIES, indent=2)}\n",
        "\n",
        "Recent conversation context:\n",
        "{context_str}\n",
        "\n",
        "Recent query history:\n",
        "{query_history_str}\n",
        "\n",
        "Current query: \"{query}\"\n",
        "\n",
        "Return ONLY a JSON object in this format:\n",
        "{{\n",
        "    \"expanded_query\": \"complete expanded query\",\n",
        "    \"topic\": \"MainTopic-SubTopic\",\n",
        "    \"confidence_score\": 0.0 to 1.0\n",
        "}}\"\"\"\n",
        "\n",
        "    def process_query(self, query: str) -> dict:\n",
        "        \"\"\"Process a query with error handling, retries, and context awareness.\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                prompt = self.create_expansion_prompt(query)\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=2000\n",
        "                )\n",
        "\n",
        "                result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "                # Validate topic and provide fallback\n",
        "                if not self.validate_topic(result[\"topic\"]):\n",
        "                    logger.warning(f\"Invalid topic format: {result['topic']}\")\n",
        "                    if result[\"confidence_score\"] < 0.75:\n",
        "                        # Fallback to a more generic topic\n",
        "                        result[\"topic\"] = self.get_fallback_topic(result[\"expanded_query\"])\n",
        "                        result[\"confidence_score\"] = 0.5\n",
        "                    else:\n",
        "                        # Use the provided topic, but log a warning\n",
        "                        result[\"topic\"] = \"General-Casual Chat\"\n",
        "                        result[\"confidence_score\"] = 0.75\n",
        "\n",
        "                # Add to conversation and query history\n",
        "                self.conversation_history.append(Message(\n",
        "                    role=\"user\",\n",
        "                    content=query,\n",
        "                    expanded_query=result[\"expanded_query\"],\n",
        "                    topic=result[\"topic\"],\n",
        "                    confidence_score=result.get(\"confidence_score\", 1.0)\n",
        "                ))\n",
        "                self.query_history.append((query, result[\"expanded_query\"], result[\"topic\"], result.get(\"confidence_score\", 1.0)))\n",
        "\n",
        "                # Maintaining a maximum of 20 query history items\n",
        "                self.query_history = self.query_history[-20:]\n",
        "\n",
        "                self.error_count = 0  # Reset error count on success\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing query (attempt {attempt + 1}): {str(e)}\")\n",
        "                self.error_count += 1\n",
        "\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    return {\n",
        "                        \"expanded_query\": query,\n",
        "                        \"topic\": \"General-Casual Chat\",\n",
        "                        \"confidence_score\": 0.3\n",
        "                    }\n",
        "\n",
        "    def get_fallback_topic(self, expanded_query: str) -> str:\n",
        "        \"\"\"Get a fallback topic based on the expanded query.\"\"\"\n",
        "\n",
        "        return \"General-Casual Chat\"\n",
        "\n",
        "    def get_bot_response(self, expanded_query: str) -> str:\n",
        "        \"\"\"Get bot response with context awareness and error handling.\"\"\"\n",
        "        try:\n",
        "            # Create context from recent conversation with topics\n",
        "            context = \"\\n\".join([\n",
        "                f\"{'User' if msg.role == 'user' else 'Assistant'}: {msg.content}\"\n",
        "                f\"{f' (Topic: {msg.topic})' if msg.topic else ''}\"\n",
        "                for msg in self.conversation_history[-3:]\n",
        "            ])\n",
        "\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{context}\n",
        "\n",
        "User question: {expanded_query}\n",
        "\n",
        "Provide a helpful, accurate, and concise response while maintaining context awareness.\"\"\"\n",
        "\n",
        "            # Use the llama_run function to get the AI output\n",
        "            bot_response = llama_run(prompt, context)\n",
        "\n",
        "            # Add to conversation history\n",
        "            self.conversation_history.append(Message(\n",
        "                role=\"assistant\",\n",
        "                content=bot_response\n",
        "            ))\n",
        "\n",
        "            return bot_response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting bot response: {str(e)}\")\n",
        "            if self.error_count > self.max_retries:\n",
        "                return \"I'm experiencing technical difficulties. Please try again later.\"\n",
        "            return \"I apologize, but I'm having trouble generating a response right now.\"\n",
        "\n",
        "    def validate_topic(self, topic: str) -> bool:\n",
        "        \"\"\"Validate if the topic follows the correct hierarchy.\"\"\"\n",
        "        try:\n",
        "            main_topic, sub_topic = topic.split('-')\n",
        "            return (main_topic in TOPIC_HIERARCHIES and\n",
        "                   sub_topic in TOPIC_HIERARCHIES[main_topic])\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "def llama_run(question, context):\n",
        "    api_key = \"f281db651f1bd4e2ca85490920a993a5d9adc509dfd78274d055f4fbe3fcc89b\"\n",
        "  #  model = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
        "    formatted_prompt = f\"What is {question} in this context: {context}\"\n",
        "    try:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=.2,\n",
        "            max_tokens=4096,\n",
        "            top_p=1,\n",
        "            top_k=40,\n",
        "            messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "            stream=True,\n",
        "        )\n",
        "        response = \"\"\n",
        "        for chunk in stream:\n",
        "            response_chunk = chunk.delta.content or \"\"\n",
        "            response += response_chunk\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Handle the error appropriately, maybe log it or return an error message\n",
        "\n",
        "def main():\n",
        "    api_key = \"f281db651f1bd4e2ca85490920a993a5d9adc509dfd78274d055f4fbe3fcc89b\"\n",
        "    processor = QueryProcessor(api_key)\n",
        "\n",
        "    # Main loop\n",
        "    while True:\n",
        "        # Getting user input\n",
        "        user_input = input(\"User: \")\n",
        "\n",
        "        # Processing the query\n",
        "        result = processor.process_query(user_input)\n",
        "        expanded_query = result[\"expanded_query\"]\n",
        "        topic = result[\"topic\"]\n",
        "        confidence_score = result.get(\"confidence_score\", 1.0)\n",
        "\n",
        "        print(f\"Expanded Query: {expanded_query}\")\n",
        "        print(f\"Topic: {topic} (Confidence: {confidence_score:.2f})\")\n",
        "\n",
        "        # Getting the bot response\n",
        "        bot_response = processor.get_bot_response(expanded_query)  # for now i have stopped it\n",
        "        print(f\"Assistant: {bot_response}\")\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bOWXDE76c48j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "3cbe7bf8-19ae-455b-a481-5b3261194041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hello how are you?\n",
            "Expanded Query: Hello, how are you?\n",
            "Topic: General-Greetings (Confidence: 0.90)\n",
            "An error occurred: name 'client' is not defined\n",
            "Assistant: None\n",
            "---\n",
            "User: tell me about nuclear power plants\n",
            "Expanded Query: Tell me about nuclear power plants and how they work\n",
            "Topic: Knowledge-Science (Confidence: 0.85)\n",
            "An error occurred: name 'client' is not defined\n",
            "Assistant: None\n",
            "---\n",
            "User: what was i asking about?\n",
            "Expanded Query: What was I asking about before, regarding nuclear power plants?\n",
            "Topic: General-Questions (Confidence: 0.95)\n",
            "An error occurred: name 'client' is not defined\n",
            "Assistant: None\n",
            "---\n",
            "User: how do they work?\n",
            "Expanded Query: How do nuclear power plants work?\n",
            "Topic: Knowledge-Science (Confidence: 0.95)\n",
            "An error occurred: name 'client' is not defined\n",
            "Assistant: None\n",
            "---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fa576ff01d8c>\u001b[0m in \u001b[0;36m<cell line: 277>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-fa576ff01d8c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Getting user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Processing the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFWpEa6GAkiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G6BC4wvAke4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2ipSYBQAkcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wD--YajZAkZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uaEdahz6AkXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cE6m8ikXAkU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ":# Dataset Genration Using llama 3.1 70b For chat Simulation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cyu4X1cS-j7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# topic hierarchies\n",
        "TOPIC_HIERARCHIES = {\n",
        "            \"Politics\": [\"Government\", \"Elections\", \"International\", \"Local\", \"Policy\"],\n",
        "            \"Technology\": [\"AI\", \"Software\", \"Hardware\", \"Internet\", \"Innovation\"],\n",
        "            \"Business\": [\"Finance\", \"Companies\", \"Economy\", \"Markets\", \"Startups\"],\n",
        "            \"Science\": [\"Physics\", \"Biology\", \"Chemistry\", \"Space\", \"Environment\"],\n",
        "            \"Entertainment\": [\"Movies\", \"Music\", \"Gaming\", \"Television\", \"Arts\"],\n",
        "            \"Sports\": [\"Worldcup\",\"Football\", \"Cricket\", \"Basketball\", \"Tennis\", \"Athletics\"],\n",
        "            \"Health\": [\"Medical\", \"Fitness\", \"Nutrition\", \"Mental\", \"Wellness\"],\n",
        "            \"Education\": [\"Academic\", \"Skills\", \"Training\", \"Research\", \"Learning\"],\n",
        "            \"Lifestyle\": [\"Food\", \"Travel\", \"Fashion\", \"Home\", \"Relationships\"],\n",
        "            \"General\": [\"Casual\", \"Greetings\", \"Small_Talk\", \"Personal\", \"Miscellaneous\"]\n",
        "}\n",
        "\n",
        "# Example conversation template for the prompt\n",
        "EXAMPLE_CONVERSATION = \"\"\"{\n",
        "  \"conversation_id\": \"conv_001\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"who is pm of india\",\n",
        "      \"expanded_query\": \"who is the prime minister of india\",\n",
        "      \"topic\": \"Politics-India\",\n",
        "      \"requires_context\": false\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"The Prime Minister of India is Narendra Modi.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"what are his main achievements\",\n",
        "      \"expanded_query\": \"what are Narendra Modi's main achievements\",\n",
        "      \"topic\": \"Politics-India\",\n",
        "      \"requires_context\": true\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Some of Narendra Modi's main achievements include implementing GST, Digital India initiative, and various economic reforms.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"gtg, will be back in 5\",\n",
        "      \"expanded_query\": \"I need to go, will be back in 5 minutes\",\n",
        "      \"topic\": \"General-Statement\",\n",
        "      \"requires_context\": false\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Sure, take your time. I'll be here when you return.\"\n",
        "    }\n",
        "  ]\n",
        "}\"\"\"\n",
        "\n",
        "#  generation prompt for the Llama model\n",
        "def create_generation_prompt(topic_focus: str = None) -> str:\n",
        "    prompt = f\"\"\"Generate a new conversation similar to the example below. The conversation should:\n",
        "1. Include natural topic transitions\n",
        "2. Use context-dependent queries\n",
        "3. Sometimes include general conversation elements\n",
        "4. Focus on {topic_focus if topic_focus else 'any topic from the available topics'}\n",
        "\n",
        "Example format:\n",
        "{EXAMPLE_CONVERSATION}\n",
        "\n",
        "Generate a new, unique conversation following this format but with different content and topics.\"\"\"\n",
        "    return prompt\n",
        "\n",
        "class DatasetGenerator:\n",
        "    def __init__(self, api_key: str, model: str, temp: float):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temp = temp\n",
        "        self.generated_conversations = []\n",
        "\n",
        "    def llama_run(self, topic_focus: str = None) -> str:\n",
        "        # prompt for a specific topic focus\n",
        "        formatted_prompt = create_generation_prompt(topic_focus)\n",
        "\n",
        "        # Simulate Together API call with generated prompt\n",
        "        client = Together(api_key=self.api_key)\n",
        "        try:\n",
        "            stream = client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                temperature=self.temp,\n",
        "                max_tokens=4096,\n",
        "                top_p=1,\n",
        "                top_k=40,\n",
        "                messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            response = \"\"\n",
        "            for chunk in stream:\n",
        "                response_chunk = chunk.choices[0].delta.content or \"\"\n",
        "                response += response_chunk\n",
        "            print(response)\n",
        "            conversation = json.loads(response)\n",
        "            print(conversation)\n",
        "            return conversation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating conversation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_dataset(self, num_conversations: int) -> list:\n",
        "        conversations = []\n",
        "\n",
        "        for _ in tqdm(range(num_conversations)):\n",
        "            topic_focus = random.choice(list(TOPIC_HIERARCHIES.keys()))\n",
        "            print(topic_focus)\n",
        "            conversation = self.llama_run(topic_focus)\n",
        "            print(conversation)\n",
        "            if conversation:\n",
        "                conversations.append(conversation)\n",
        "\n",
        "        self.generated_conversations = conversations\n",
        "        return conversations\n",
        "\n",
        "    def save_dataset(self, filename: str = \"conversation_dataset.json\"):\n",
        "        # Save as JSON\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.generated_conversations, f, indent=2)\n",
        "\n",
        "        # Also saving as CSV for easier inspection\n",
        "        flat_data = []\n",
        "        for conv in self.generated_conversations:\n",
        "            for msg in conv['messages']:\n",
        "                if msg['role'] == 'user':\n",
        "                    flat_data.append({\n",
        "                        'conversation_id': conv['conversation_id'],\n",
        "                        'query': msg['content'],\n",
        "                        'expanded_query': msg.get('expanded_query', ''),\n",
        "                        'topic': msg.get('topic', ''),\n",
        "                        'requires_context': msg.get('requires_context', False)\n",
        "                    })\n",
        "\n",
        "        df = pd.DataFrame(flat_data)\n",
        "        df.to_csv(filename.replace('.json', '.csv'), index=False)\n",
        "\n",
        "# Usage\n",
        "def main():\n",
        "    generator = DatasetGenerator(api_key=\"f281db651f1bd4e2ca85490920a993a5d9adc509dfd78274d055f4fbe3fcc89b\", model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", temp=0.2)\n",
        "    conversations = generator.generate_dataset(100)  # Generate 100 conversations\n",
        "\n",
        "    # Saving the dataset\n",
        "    generator.save_dataset()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "TNgAptfu-O0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Define topic hierarchies (unchanged)\n",
        "TOPIC_HIERARCHIES = {\n",
        "    \"Politics\": [\"India\", \"UK\", \"USA\", \"China\", \"Global\"],\n",
        "    \"Sports\": [\"Cricket\", \"Football\", \"Tennis\", \"Basketball\", \"Olympics\"],\n",
        "    \"Technology\": [\"AI\", \"Mobile\", \"Gaming\", \"Internet\", \"Gadgets\"],\n",
        "    \"Entertainment\": [\"Movies\", \"Music\", \"TV\", \"Celebrity\", \"Gaming\"],\n",
        "    \"Business\": [\"Finance\", \"Startups\", \"Markets\", \"Economy\", \"Companies\"],\n",
        "    \"Science\": [\"Space\", \"Medicine\", \"Environment\", \"Research\", \"Innovation\"],\n",
        "    \"General\": [\"Greeting\", \"Casual\", \"Question\", \"Statement\"]\n",
        "}\n",
        "\n",
        "# Modified conversation template with context tracking\n",
        "EXAMPLE_CONVERSATION = \"\"\"{\n",
        "  \"conversation_id\": \"conv_001\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"who is pm of india\",\n",
        "      \"expanded_query\": \"who is the prime minister of india\",\n",
        "      \"topic\": \"Politics-India\",\n",
        "      \"requires_context\": false,\n",
        "      \"context_messages\": []\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"The Prime Minister of India is Narendra Modi.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"what are his main achievements\",\n",
        "      \"expanded_query\": \"what are Narendra Modi's main achievements\",\n",
        "      \"topic\": \"Politics-India\",\n",
        "      \"requires_context\": true,\n",
        "      \"context_messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"who is pm of india\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Prime Minister of India is Narendra Modi.\"}\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\"\"\"\n",
        "\n",
        "def create_generation_prompt(topic_focus: str = None) -> str:\n",
        "    prompt = f\"\"\"Generate a new conversation similar to the example below. The conversation should:\n",
        "1. Include natural topic transitions\n",
        "2. Use context-dependent queries that reference previous messages\n",
        "3. Sometimes include general conversation elements\n",
        "4. Have at least 20 messages (10 exchanges between user and assistant)\n",
        "5. Focus on {topic_focus if topic_focus else 'any topic from the available topics'}\n",
        "\n",
        "Example format:\n",
        "{EXAMPLE_CONVERSATION}\n",
        "\n",
        "Generate a new, unique conversation following this format but with different content and topics.\n",
        "Make sure to maintain proper context tracking for each user message.\"\"\"\n",
        "    return prompt\n",
        "\n",
        "class DatasetGenerator:\n",
        "    def __init__(self, api_key: str, model: str, temp: float):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temp = temp\n",
        "        self.generated_conversations = []\n",
        "\n",
        "    def get_context_messages(self, messages: List[Dict[str, Any]], current_idx: int,\n",
        "                           context_window: int = 20) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get the previous messages as context for the current message.\n",
        "\n",
        "        Args:\n",
        "            messages: List of all messages in the conversation\n",
        "            current_idx: Index of the current message\n",
        "            context_window: Number of previous messages to include as context\n",
        "\n",
        "        Returns:\n",
        "            List of context messages\n",
        "        \"\"\"\n",
        "        context_start = max(0, current_idx - context_window)\n",
        "        return messages[context_start:current_idx]\n",
        "\n",
        "    def process_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a generated conversation to add proper context tracking.\n",
        "        \"\"\"\n",
        "        messages = conversation['messages']\n",
        "        context_queue = deque(maxlen=20)  # Keep track of last 20 messages\n",
        "\n",
        "        for i, message in enumerate(messages):\n",
        "            if message['role'] == 'user':\n",
        "                # Add context tracking for user messages\n",
        "                message['context_messages'] = list(context_queue)\n",
        "\n",
        "                # Determine if the message requires context\n",
        "                message['requires_context'] = any(\n",
        "                    word in message['content'].lower()\n",
        "                    for word in ['this', 'that', 'it', 'they', 'he', 'she', 'their', 'these', 'those']\n",
        "                ) or i > 0\n",
        "\n",
        "            # Add message to context queue\n",
        "            context_queue.append({\n",
        "                'role': message['role'],\n",
        "                'content': message['content']\n",
        "            })\n",
        "\n",
        "        return conversation\n",
        "\n",
        "    def llama_run(self, topic_focus: str = None) -> str:\n",
        "        formatted_prompt = create_generation_prompt(topic_focus)\n",
        "\n",
        "        # Simulate Together API call with generated prompt\n",
        "        client = Together(api_key=self.api_key)\n",
        "        try:\n",
        "            stream = client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                temperature=self.temp,\n",
        "                max_tokens=4096,  # Increased to accommodate longer conversations\n",
        "                top_p=1,\n",
        "                top_k=40,\n",
        "                messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            response = \"\"\n",
        "            for chunk in stream:\n",
        "                response_chunk = chunk.choices[0].delta.content or \"\"\n",
        "                response += response_chunk\n",
        "\n",
        "            conversation = json.loads(response)\n",
        "            processed_conversation = self.process_conversation(conversation)\n",
        "            return processed_conversation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating conversation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_dataset(self, num_conversations: int) -> list:\n",
        "        conversations = []\n",
        "\n",
        "        for _ in tqdm(range(num_conversations)):\n",
        "            topic_focus = random.choice(list(TOPIC_HIERARCHIES.keys()))\n",
        "            print(f\"Generating conversation with topic focus: {topic_focus}\")\n",
        "\n",
        "            conversation = self.llama_run(topic_focus)\n",
        "            if conversation:\n",
        "                # Verify conversation length\n",
        "                if len(conversation['messages']) >= 10:\n",
        "                    conversations.append(conversation)\n",
        "                else:\n",
        "                    print(f\"Skipping conversation with insufficient messages: {len(conversation['messages'])}\")\n",
        "\n",
        "        self.generated_conversations = conversations\n",
        "        return conversations\n",
        "\n",
        "    def save_dataset(self, filename: str = \"conversation_dataset.json\"):\n",
        "        # Save as JSON\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.generated_conversations, f, indent=2)\n",
        "\n",
        "        # Save as CSV with context information\n",
        "        flat_data = []\n",
        "        for conv in self.generated_conversations:\n",
        "            for msg in conv['messages']:\n",
        "                if msg['role'] == 'user':\n",
        "                    flat_data.append({\n",
        "                        'conversation_id': conv['conversation_id'],\n",
        "                        'query': msg['content'],\n",
        "                        'expanded_query': msg.get('expanded_query', ''),\n",
        "                        'topic': msg.get('topic', ''),\n",
        "                        'requires_context': msg.get('requires_context', False),\n",
        "                        'context_messages': json.dumps(msg.get('context_messages', []), ensure_ascii=False),\n",
        "                        'num_context_messages': len(msg.get('context_messages', [])),\n",
        "                    })\n",
        "\n",
        "        df = pd.DataFrame(flat_data)\n",
        "        df.to_csv(filename.replace('.json', '.csv'), index=False)\n",
        "\n",
        "def main():\n",
        "    generator = DatasetGenerator(\n",
        "        api_key=\"f281db651f1bd4e2ca85490920a993a5d9adc509dfd78274d055f4fbe3fcc89b\",\n",
        "        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "        temp=0.2\n",
        "    )\n",
        "    conversations = generator.generate_dataset(num_conversations=50)\n",
        "    generator.save_dataset()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQXLdAOxfoxc",
        "outputId": "165b6e76-e3c0-44f8-be65-308e4acc42a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: General\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 1/50 [00:17<14:24, 17.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:33<13:24, 16.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [00:46<11:32, 14.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [00:55<09:40, 12.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [01:10<10:01, 13.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [01:28<10:57, 14.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: General\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [01:47<11:45, 16.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: General\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [02:00<10:36, 15.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [02:12<09:44, 14.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [02:24<09:09, 13.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 11/50 [02:37<08:47, 13.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [02:51<08:33, 13.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating conversation: Expecting value: line 1 column 1 (char 0)\n",
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [03:03<08:05, 13.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [03:19<08:16, 13.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [03:35<08:35, 14.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [03:53<08:50, 15.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 17/50 [04:09<08:35, 15.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Business\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 18/50 [04:27<08:44, 16.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 19/50 [04:38<07:34, 14.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Business\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 20/50 [04:48<06:41, 13.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Politics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 21/50 [05:13<08:10, 16.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Politics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [05:26<07:23, 15.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: General\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 23/50 [05:36<06:15, 13.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: General\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 24/50 [06:04<07:49, 18.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Politics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 25/50 [06:17<06:57, 16.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 26/50 [06:31<06:21, 15.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 27/50 [06:43<05:37, 14.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 28/50 [06:55<05:06, 13.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 29/50 [07:12<05:10, 14.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 30/50 [07:26<04:52, 14.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating conversation: Expecting value: line 1 column 1 (char 0)\n",
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 31/50 [07:38<04:20, 13.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 32/50 [07:49<03:51, 12.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 33/50 [08:04<03:49, 13.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Politics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 34/50 [08:18<03:42, 13.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: General\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 35/50 [08:44<04:20, 17.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 36/50 [08:55<03:37, 15.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 37/50 [09:12<03:25, 15.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 38/50 [09:30<03:20, 16.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 39/50 [09:45<02:57, 16.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 40/50 [10:05<02:52, 17.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Politics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 41/50 [10:19<02:26, 16.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 42/50 [10:32<02:02, 15.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 43/50 [10:44<01:40, 14.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 44/50 [10:58<01:24, 14.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 45/50 [11:16<01:17, 15.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 46/50 [11:29<00:58, 14.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Entertainment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 47/50 [11:39<00:39, 13.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Business\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 48/50 [11:49<00:24, 12.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 49/50 [12:04<00:13, 13.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversation with topic focus: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [12:21<00:00, 14.83s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Genration Using llama 3.1 70b For chat Simulation\n",
        "\n"
      ],
      "metadata": {
        "id": "nekHgTUIlKOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generation prompt for the Llama model\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# Define topic hierarchies (unchanged)\n",
        "# Define topic hierarchies (unchanged)\n",
        "TOPIC_HIERARCHIES = {\n",
        "    \"Politics\": [\"India\", \"UK\", \"USA\", \"China\", \"Global\"],\n",
        "    \"Sports\": [\"Cricket\", \"Football\", \"Tennis\", \"Basketball\", \"Olympics\"],\n",
        "    \"Technology\": [\"AI\", \"Mobile\", \"Gaming\", \"Internet\", \"Gadgets\"],\n",
        "    \"Entertainment\": [\"Movies\", \"Music\", \"TV\", \"Celebrity\", \"Gaming\"],\n",
        "    \"Business\": [\"Finance\", \"Startups\", \"Markets\", \"Economy\", \"Companies\"],\n",
        "    \"Science\": [\"Space\", \"Medicine\", \"Environment\", \"Research\", \"Innovation\"],\n",
        "    \"General\": [\"Greeting\", \"Casual\", \"Question\", \"Statement\"]\n",
        "}\n",
        "\n",
        "EXAMPLE_CONVERSATION =\"\"\"{\n",
        "  \"conversation_id\": \"conv_001\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"who is pm of india\",\n",
        "      \"expanded_query\": \"who is the prime minister of india?\",\n",
        "      \"requires_context\": false\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"what are his main achievements?\",\n",
        "      \"expanded_query\": \"what are the main achievements of the prime minister of india?\",\n",
        "      \"requires_context\": true\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Okay bye, will talk later\",\n",
        "      \"expanded_query\": \"Okay bye, will talk later\",\n",
        "      \"requires_context\": false\n",
        "    },\n",
        "\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What about Pakistan?\",\n",
        "      \"expanded_query\": \"Who is the pm of Pakistan and what are his main achievement?\",\n",
        "      \"requires_context\": True\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What is 2+2?\",\n",
        "      \"expanded_query\": \"What is 2+2?\",\n",
        "      \"requires_context\": false\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "def create_generation_prompt(topic_focus: str = None) -> str:\n",
        "    prompt = f\"\"\"Generate a new conversation similar to the example below. The conversation should:\n",
        "1. Include only user messages.\n",
        "2. Expand queries based on context from previous user queries when necessary.\n",
        "3. Sometimes include general conversation elements\n",
        "4. Have at least 20 messages\n",
        "5. Avoid assistant responses entirely.\n",
        "6. Focus on {topic_focus if topic_focus else 'any topic from the available topics'}\n",
        "\n",
        "Example format:\n",
        "{EXAMPLE_CONVERSATION}\n",
        "\n",
        "Generate a new, unique conversation following this format but with different content and topics.\"\"\"\n",
        "    return prompt\n",
        "class DatasetGenerator:\n",
        "    def __init__(self, api_key: str, model: str, temp: float):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temp = temp\n",
        "        self.generated_conversations = []\n",
        "\n",
        "    def llama_run(self, topic_focus: str = None) -> str:\n",
        "        # Generate prompt for a specific topic focus\n",
        "        formatted_prompt = create_generation_prompt(topic_focus)\n",
        "\n",
        "        # Simulate Together API call with generated prompt\n",
        "        client = Together(api_key=self.api_key)\n",
        "        try:\n",
        "            stream = client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                temperature=self.temp,\n",
        "                max_tokens=4096,\n",
        "                top_p=1,\n",
        "                top_k=40,\n",
        "                messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            response = \"\"\n",
        "            for chunk in stream:\n",
        "                response_chunk = chunk.choices[0].delta.content or \"\"\n",
        "                response += response_chunk\n",
        "\n",
        "            # Parse the conversation, filtering out any assistant content\n",
        "            conversation = json.loads(response)\n",
        "            filtered_conversation = {\n",
        "                \"conversation_id\": conversation[\"conversation_id\"],\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": msg[\"role\"],\n",
        "                        \"content\": msg[\"content\"],\n",
        "                        \"expanded_query\": msg[\"expanded_query\"],\n",
        "                        \"requires_context\": msg[\"requires_context\"]\n",
        "                    }\n",
        "                    for msg in conversation[\"messages\"]\n",
        "                    if msg[\"role\"] == \"user\"\n",
        "                ]\n",
        "            }\n",
        "            return filtered_conversation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating conversation: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_dataset(self, num_conversations: int) -> list:\n",
        "      conversations = []\n",
        "      topic_subtopics = [(topic, subtopic) for topic, subtopics in TOPIC_HIERARCHIES.items() for subtopic in subtopics]\n",
        "\n",
        "      # Determine how many conversations per topic-subtopic pair\n",
        "      total_combinations = len(topic_subtopics)\n",
        "      num_per_combination = max(1, num_conversations // total_combinations)\n",
        "\n",
        "      for topic, subtopic in topic_subtopics:\n",
        "          print(f\"Generating conversations for topic: {topic}, subtopic: {subtopic}\")\n",
        "          for _ in range(num_per_combination):\n",
        "              topic_focus = f\"{topic} - {subtopic}\"\n",
        "              conversation = self.llama_run(topic_focus)\n",
        "              if conversation:\n",
        "                  conversations.append(conversation)\n",
        "\n",
        "      # Generate remaining conversations if num_conversations is not perfectly divisible\n",
        "      remaining_conversations = num_conversations - len(conversations)\n",
        "      if remaining_conversations > 0:\n",
        "          print(\"Generating remaining conversations to match total count.\")\n",
        "          for _ in range(remaining_conversations):\n",
        "              topic, subtopic = random.choice(topic_subtopics)\n",
        "              topic_focus = f\"{topic} - {subtopic}\"\n",
        "              conversation = self.llama_run(topic_focus)\n",
        "              if conversation:\n",
        "                  conversations.append(conversation)\n",
        "\n",
        "      self.generated_conversations = conversations\n",
        "      return conversations\n",
        "\n",
        "\n",
        "    def save_dataset(self, filename: str = \"conversation_dataset.json\"):\n",
        "        # Save as JSON\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.generated_conversations, f, indent=2)\n",
        "\n",
        "        # Also saving as CSV for easier inspection\n",
        "        flat_data = []\n",
        "        for conv in self.generated_conversations:\n",
        "            for msg in conv['messages']:\n",
        "                if msg['role'] == 'user':\n",
        "                    flat_data.append({\n",
        "                        'conversation_id': conv['conversation_id'],\n",
        "                        'query': msg['content'],\n",
        "                        'expanded_query': msg.get('expanded_query', ''),\n",
        "                        'topic': msg.get('topic', ''),\n",
        "                        'requires_context': msg.get('requires_context', False)\n",
        "                    })\n",
        "\n",
        "        df = pd.DataFrame(flat_data)\n",
        "        df.to_csv(filename.replace('.json', '.csv'), index=False)\n",
        "\n",
        "# Usage\n",
        "def main():\n",
        "    generator = DatasetGenerator(api_key=\"f281db651f1bd4e2ca85490920a993a5d9adc509dfd78274d055f4fbe3fcc89b\", model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", temp=0.2)\n",
        "    conversations = generator.generate_dataset(1)  # Generate 100 conversations\n",
        "\n",
        "    # Saving the dataset\n",
        "    generator.save_dataset()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Ul4NtF7Z7WTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2bac11c-6ae2-4615-d4c0-90a1631115ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating conversations for topic: Politics, subtopic: India\n",
            "Generating conversations for topic: Politics, subtopic: UK\n",
            "Generating conversations for topic: Politics, subtopic: USA\n",
            "Generating conversations for topic: Politics, subtopic: China\n",
            "Generating conversations for topic: Politics, subtopic: Global\n",
            "Generating conversations for topic: Sports, subtopic: Cricket\n",
            "Generating conversations for topic: Sports, subtopic: Football\n",
            "Generating conversations for topic: Sports, subtopic: Tennis\n",
            "Generating conversations for topic: Sports, subtopic: Basketball\n",
            "Generating conversations for topic: Sports, subtopic: Olympics\n",
            "Generating conversations for topic: Technology, subtopic: AI\n",
            "Generating conversations for topic: Technology, subtopic: Mobile\n",
            "Generating conversations for topic: Technology, subtopic: Gaming\n",
            "Generating conversations for topic: Technology, subtopic: Internet\n",
            "Generating conversations for topic: Technology, subtopic: Gadgets\n",
            "Generating conversations for topic: Entertainment, subtopic: Movies\n",
            "Generating conversations for topic: Entertainment, subtopic: Music\n",
            "Generating conversations for topic: Entertainment, subtopic: TV\n",
            "Error generating conversation: Expecting value: line 86 column 27 (char 2598)\n",
            "Generating conversations for topic: Entertainment, subtopic: Celebrity\n",
            "Generating conversations for topic: Entertainment, subtopic: Gaming\n",
            "Generating conversations for topic: Business, subtopic: Finance\n",
            "Generating conversations for topic: Business, subtopic: Startups\n",
            "Generating conversations for topic: Business, subtopic: Markets\n",
            "Generating conversations for topic: Business, subtopic: Economy\n",
            "Generating conversations for topic: Business, subtopic: Companies\n",
            "Generating conversations for topic: Science, subtopic: Space\n",
            "Generating conversations for topic: Science, subtopic: Medicine\n",
            "Generating conversations for topic: Science, subtopic: Environment\n",
            "Generating conversations for topic: Science, subtopic: Research\n",
            "Generating conversations for topic: Science, subtopic: Innovation\n",
            "Generating conversations for topic: General, subtopic: Greeting\n",
            "Generating conversations for topic: General, subtopic: Casual\n",
            "Generating conversations for topic: General, subtopic: Question\n",
            "Generating conversations for topic: General, subtopic: Statement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "yRtyB36uRbFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "    expanded_query: str\n",
        "    requires_context: bool\n",
        "\n",
        "@dataclass\n",
        "class Conversation:\n",
        "    conversation_id: str\n",
        "    messages: List[Message]\n",
        "\n",
        "def process_conversations(raw_data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Process raw conversation data into training examples.\n",
        "    Each example includes conversation history and the target expanded query.\n",
        "    \"\"\"\n",
        "    training_examples = []\n",
        "\n",
        "    for conv in raw_data:\n",
        "        messages = conv['messages']\n",
        "        conversation_history = []\n",
        "\n",
        "        for idx, msg in enumerate(messages):\n",
        "            if msg['role'] != 'user':\n",
        "                continue\n",
        "\n",
        "            # Create the conversation history\n",
        "            history = \"\"\n",
        "            if conversation_history:\n",
        "                history = \" [SEP] \".join(conversation_history)\n",
        "\n",
        "            # Create training example\n",
        "            example = {\n",
        "                'conversation_id': conv['conversation_id'],\n",
        "                'input_query': msg['content'],\n",
        "                'conversation_history': history,\n",
        "                'expanded_query': msg['expanded_query'],\n",
        "                'requires_context': msg['requires_context']\n",
        "            }\n",
        "\n",
        "            training_examples.append(example)\n",
        "            conversation_history.append(f\"User: {msg['content']}\")\n",
        "\n",
        "    return training_examples\n",
        "\n",
        "def create_huggingface_dataset(training_examples: List[Dict]) -> Dataset:\n",
        "    \"\"\"\n",
        "    Convert the processed examples into a Hugging Face dataset.\n",
        "    \"\"\"\n",
        "    dataset_dict = {\n",
        "        'conversation_id': [],\n",
        "        'input_query': [],\n",
        "        'conversation_history': [],\n",
        "        'expanded_query': [],\n",
        "        'requires_context': []\n",
        "    }\n",
        "\n",
        "    for example in training_examples:\n",
        "        for key in dataset_dict.keys():\n",
        "            dataset_dict[key].append(example[key])\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "def format_for_training(dataset: Dataset) -> Dataset:\n",
        "    \"\"\"\n",
        "    Format the dataset for training by creating input and target text fields.\n",
        "    \"\"\"\n",
        "    def format_example(example):\n",
        "        # Format input text\n",
        "        input_text = f\"Query: {example['input_query']}\\n\"\n",
        "        if example['conversation_history']:\n",
        "            input_text = f\"Context: {example['conversation_history']}\\n\" + input_text\n",
        "\n",
        "        # Format target text\n",
        "        target_text = example['expanded_query']\n",
        "\n",
        "        return {\n",
        "            'input_text': input_text,\n",
        "            'target_text': target_text\n",
        "        }\n",
        "\n",
        "    return dataset.map(format_example)\n",
        "\n",
        "# Example usage:\n",
        "# Load your JSON data\n",
        "with open('/content/conversation_dataset (2).json', 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Process the conversations\n",
        "training_examples = process_conversations(raw_data)\n",
        "\n",
        "# Create Hugging Face dataset\n",
        "dataset = create_huggingface_dataset(training_examples)\n",
        "\n",
        "# Format for training\n",
        "training_dataset = format_for_training(dataset)\n",
        "\n",
        "# Save the dataset\n",
        "training_dataset.save_to_disk('query_expansion_dataset')\n"
      ],
      "metadata": {
        "id": "UMNtkc60fIE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "6b54a11428d240288638e2259a2e5528",
            "d902dc15d6644b588e013253d4669ce5",
            "e423feaa45dc48b78369e8ac73ccc3b8",
            "f1910c2094e84bb298317a74f21f7381",
            "8691862123f7413f9170690e4d594edd",
            "09a954b674ea481f83792ac79a1ea1a8",
            "2094f8b5fe97433f92320f03fedc29e7",
            "0b796a06ce864a019f8adc19976605b1",
            "a3a1113d516d4be0a5f9bcd80059ff4b",
            "3ac519fed7ee48179fec7c649317fb2d",
            "f2084e8c3d8d4010ab4d8b4a8e4c5b2b",
            "bcecf43e3bcf4a809f80da69f44ec17d",
            "02101b1d4b89410b920412e83bd3507e",
            "031c8dc5b4194524af786827a53be49f",
            "437b699d6d624d24b8672320f9e393bf",
            "d3e2520a7f5b48cd8feace85f9d6a590",
            "1b645e428cc44fba951c21439fa74883",
            "cfc8f33f1424471db17c1ddd5a03355b",
            "e539591591c0465da1d5a9587a8adf8d",
            "d91ae2c25c5f4ca18019fe2aedb0381e",
            "2fd7214c8dc34da886883f6bc04d521a",
            "682c853356104f8ab999767da9e3c85c"
          ]
        },
        "outputId": "55a35774-ff45-4e03-ffa3-f2f0ab592433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b54a11428d240288638e2259a2e5528"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/653 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcecf43e3bcf4a809f80da69f44ec17d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = datasets.load_from_disk('query_expansion_dataset')"
      ],
      "metadata": {
        "id": "5JondCU9RMIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgcUiIvLRxjz",
        "outputId": "fe1b2026-d891-4230-9e23-ce4b3ecb9e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversation_id', 'input_query', 'conversation_history', 'expanded_query', 'requires_context', 'input_text', 'target_text'],\n",
              "    num_rows: 653\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print two example dataset from the abaove\n",
        "dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD1eGIZcRzSF",
        "outputId": "622d90bf-262b-4c5e-f911-a7f7dd6479e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversation_id': 'conv_002',\n",
              " 'input_query': 'can he be removed',\n",
              " 'conversation_history': 'User: who is the president of india [SEP] User: what are his powers [SEP] User: can he dissolve parliament [SEP] User: what about the lok sabha [SEP] User: who is the speaker of lok sabha [SEP] User: what are his responsibilities [SEP] User: how is he elected [SEP] User: what about the rajya sabha [SEP] User: who is the chairman of rajya sabha [SEP] User: what are his powers',\n",
              " 'expanded_query': 'can the chairman of the rajya sabha be removed from office?',\n",
              " 'requires_context': True,\n",
              " 'input_text': 'Context: User: who is the president of india [SEP] User: what are his powers [SEP] User: can he dissolve parliament [SEP] User: what about the lok sabha [SEP] User: who is the speaker of lok sabha [SEP] User: what are his responsibilities [SEP] User: how is he elected [SEP] User: what about the rajya sabha [SEP] User: who is the chairman of rajya sabha [SEP] User: what are his powers\\nQuery: can he be removed\\n',\n",
              " 'target_text': 'can the chairman of the rajya sabha be removed from office?'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from typing import Dict, List\n",
        "\n",
        "# Define the Alpaca prompt template\n",
        "ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Expand the given query based on the conversation context if available.\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "\n",
        "def prepare_dataset_for_alpaca(examples: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Converts the query expansion dataset into Alpaca format.\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "\n",
        "    for input_text, target_text in zip(examples['input_text'], examples['target_text']):\n",
        "        # Format the text according to Alpaca template\n",
        "        text = ALPACA_PROMPT.format(\n",
        "            input=input_text.strip(),\n",
        "            output=target_text.strip()\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "def format_dataset(dataset: Dataset, tokenizer) -> Dataset:\n",
        "    \"\"\"\n",
        "    Format the dataset and add EOS tokens.\n",
        "    \"\"\"\n",
        "    # First convert to Alpaca format\n",
        "    formatted_dataset = dataset.map(\n",
        "        prepare_dataset_for_alpaca,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names  # Remove original columns\n",
        "    )\n",
        "\n",
        "    # Add EOS token to each example\n",
        "    def add_eos_token(example):\n",
        "        example['text'] = example['text'] + tokenizer.eos_token\n",
        "        return example\n",
        "\n",
        "    formatted_dataset = formatted_dataset.map(add_eos_token)\n",
        "\n",
        "    return formatted_dataset\n",
        "\n",
        "# Example usage:\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load your dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    'conversation_id': [...],\n",
        "    'input_text': [...],\n",
        "    'target_text': [...],\n",
        "    'requires_context': [...]\n",
        "})\n",
        "\n",
        "# Load your tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your-model-name\")\n",
        "\n",
        "# Format the dataset\n",
        "formatted_dataset = format_dataset(dataset, tokenizer)\n",
        "\n",
        "# Now your dataset is ready for fine-tuning!\n",
        "\"\"\"\n",
        "\n",
        "# For demonstration, here's how to process a single example:\n",
        "def format_single_example(example: Dict, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Format a single example to show the output format.\n",
        "    \"\"\"\n",
        "    text = ALPACA_PROMPT.format(\n",
        "        input=example['input_text'].strip(),\n",
        "        output=example['target_text'].strip()\n",
        "    )\n",
        "    return text + tokenizer.eos_token\n",
        "\n",
        "# Example of how the formatting looks\n",
        "example = {\n",
        "    'input_text': 'Context: User: who is the president of india [SEP] User: what are his powers\\nQuery: can he dissolve parliament\\n',\n",
        "    'target_text': 'can the president of india dissolve the parliament?'\n",
        "}\n",
        "\n",
        "print(\"Example of formatted output:\")\n",
        "print(\"-\" * 50)\n",
        "print(format_single_example(example, tokenizer))  # Note: This won't run without a tokenizer"
      ],
      "metadata": {
        "id": "-on3kd5TSMs8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b54a11428d240288638e2259a2e5528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d902dc15d6644b588e013253d4669ce5",
              "IPY_MODEL_e423feaa45dc48b78369e8ac73ccc3b8",
              "IPY_MODEL_f1910c2094e84bb298317a74f21f7381"
            ],
            "layout": "IPY_MODEL_8691862123f7413f9170690e4d594edd"
          }
        },
        "d902dc15d6644b588e013253d4669ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a954b674ea481f83792ac79a1ea1a8",
            "placeholder": "​",
            "style": "IPY_MODEL_2094f8b5fe97433f92320f03fedc29e7",
            "value": "Map: 100%"
          }
        },
        "e423feaa45dc48b78369e8ac73ccc3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b796a06ce864a019f8adc19976605b1",
            "max": 653,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3a1113d516d4be0a5f9bcd80059ff4b",
            "value": 653
          }
        },
        "f1910c2094e84bb298317a74f21f7381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ac519fed7ee48179fec7c649317fb2d",
            "placeholder": "​",
            "style": "IPY_MODEL_f2084e8c3d8d4010ab4d8b4a8e4c5b2b",
            "value": " 653/653 [00:00&lt;00:00, 3296.70 examples/s]"
          }
        },
        "8691862123f7413f9170690e4d594edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a954b674ea481f83792ac79a1ea1a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2094f8b5fe97433f92320f03fedc29e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b796a06ce864a019f8adc19976605b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a1113d516d4be0a5f9bcd80059ff4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ac519fed7ee48179fec7c649317fb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2084e8c3d8d4010ab4d8b4a8e4c5b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcecf43e3bcf4a809f80da69f44ec17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02101b1d4b89410b920412e83bd3507e",
              "IPY_MODEL_031c8dc5b4194524af786827a53be49f",
              "IPY_MODEL_437b699d6d624d24b8672320f9e393bf"
            ],
            "layout": "IPY_MODEL_d3e2520a7f5b48cd8feace85f9d6a590"
          }
        },
        "02101b1d4b89410b920412e83bd3507e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b645e428cc44fba951c21439fa74883",
            "placeholder": "​",
            "style": "IPY_MODEL_cfc8f33f1424471db17c1ddd5a03355b",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "031c8dc5b4194524af786827a53be49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e539591591c0465da1d5a9587a8adf8d",
            "max": 653,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d91ae2c25c5f4ca18019fe2aedb0381e",
            "value": 653
          }
        },
        "437b699d6d624d24b8672320f9e393bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd7214c8dc34da886883f6bc04d521a",
            "placeholder": "​",
            "style": "IPY_MODEL_682c853356104f8ab999767da9e3c85c",
            "value": " 653/653 [00:00&lt;00:00, 13496.74 examples/s]"
          }
        },
        "d3e2520a7f5b48cd8feace85f9d6a590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b645e428cc44fba951c21439fa74883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc8f33f1424471db17c1ddd5a03355b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e539591591c0465da1d5a9587a8adf8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d91ae2c25c5f4ca18019fe2aedb0381e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fd7214c8dc34da886883f6bc04d521a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682c853356104f8ab999767da9e3c85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}